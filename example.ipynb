{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Multi-instance (MI) machine learning approaches can be used to solve the issues of representation of each molecule by multiple conformations (instances) and automatic selection of the most relevant ones. In the multi-instance approach, an example (i.e., a molecule) is presented by a bag of instances (i.e., a set of conformations), and a label (a molecule property value) is available only for a bag (a molecule), but not for individual instances (conformations).\n",
    "\n",
    "Here, we report an application of Multi-Instance Learning approach to predictive modeling of enantioselectivity of chiral catalysts. Catalysts were represented by ensembles of conformers encoded by the pmapper physicochemical descriptors capturing stereo configuration of the molecule. Each catalyzed chemical reaction was transformed to a Condensed Graph of Reaction for which ISIDA fragment descriptors were generated. This approach does not require any conformers’ alignment and can potentially be used for diverse set of catalysts bearing different scaffolds.\n",
    "\n",
    "<img src=\"img/approach.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptors\n",
    "\n",
    "Each reaction was transformed to a Condensed Graph of Reaction (CGR) with a CGRtools package. CGR is a single graph, which encodes an ensemble of reactants and products. CGR results from the superposition of the atoms of products and reactants having the same numbers. It contains both conventional chemical bonds (single, double, triple, aromatic, etc.) and so-called “dynamic” bonds describing chemical transformations, i.e. breaking or forming a bond or changing bond order. Given CGRs were encoded by ISIDA (In Silico Design and Data Analysis) fragment descriptors, counting the occurrence of particular subgraphs (structural fragments) of different topologies and sizes. In this study, atom-centered subgraphs containing a given atom with the atoms and bonds of its n coordination spheres (n = 1-4) are used.\n",
    "\n",
    "<img src=\"img/cgr.png\" width=\"500\"/>\n",
    "\n",
    "For each catalyst, up to 50 conformations (**nconfs**) within a 10 kcal/mol energy window (**energy**) have been generated using the distance geometry algorithm implemented in RDKit13. The conformations with RMSD values below 0.5Å with respect to selected conformers were removed in order to reduce redundancy. Then, selected conformers were encoded by a vector of pmapper descriptors. Each conformer is represented by an ensemble of physicochemical features assigned to atoms, functional groups, or rings: H-donor, H-acceptor, or hydrophobic, or positively or negatively charged. Rings are characterized by either hydrophobic or aromatic features. All possible combinations of features quadruplets are enumerated. Each quadruplet is encoded by a canonical signature, which contains information about comprising features, the distance between them, and stereoconfiguration. To enable fuzzy matching of quadruplets to identify similar ones, the distances between features are binned with the step of 1Å. Each unique quadruplet is considered as a descriptor whereas its count is a descriptor value. \n",
    "<img src=\"img/pmapper_descriptors.png\" width=\"900\"/>\n",
    "\n",
    "Vectors of 2D fragment reaction descriptors and 3D physicochemical quadruplets were then concatenated to form combined reaction/catalyst descriptor vector. \n",
    "<img src=\"img/isida_pmapper_descriptors.png\" width=\"800\"/>\n",
    "\n",
    "The descriptor calculation function reads an RDF file with reactions where the CATALYST_SMILES field contains the catalyst smiles. The SELECTIVITY field stores the experimental value of the selectivity (ΔΔG) in the reaction. The ID field contains a unique reaction index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from miqssr.utils import calc_descriptors\n",
    "\n",
    "input_fname = os.path.join('data', 'input_data.rdf')\n",
    "nconfs = 50 # max number of conformers to generate\n",
    "energy = 10 # energy window\n",
    "ncpu = 20 # number of cpus\n",
    "path = './descriptors' # where to store the calculated descriptors\n",
    "\n",
    "out_fname = calc_descriptors(input_fname=input_fname, nconfs=nconfs, energy=energy, ncpu=ncpu, path=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "The descriptors file contains columns: *react_id* (reaction index), *mol_title* (reaction name), *act* - selectivity of reaction.\n",
    "\n",
    "One should to implement a function to create a n × m × k list of bags (n - number of reactions, m - bag size (number of conformers generated), k - number of descriptors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(fname):\n",
    "    #data = pd.read_csv(fname, index_col='react_id').sort_index()\n",
    "    data = pd.read_csv(fname, index_col='mol_id').sort_index()\n",
    "    bags, labels, idx = [], [], []\n",
    "    for i in data.index.unique():\n",
    "        bag = data.loc[i:i].drop(['mol_title', 'act'], axis=1).values\n",
    "        label = float(data.loc[i:i]['act'].unique()[0])\n",
    "\n",
    "        bags.append(bag)\n",
    "        labels.append(label)\n",
    "        idx.append(i)\n",
    "\n",
    "    return np.array(bags), np.array(labels), idx\n",
    "\n",
    "\n",
    "dsc_fname = os.path.join('descriptors', 'PhFprPmapper_concat-data_50.csv') # descriptors file\n",
    "bags, labels, idx = load_data(dsc_fname)\n",
    "print(f'There are {len(bags)} reactions encoded with {bags[0].shape[1]} descriptors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set was 384 reactions (24 catalysts × 16 substrate combinations = 384 reactions), and the external test set was composed of the 691 reactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_default(bags, labels, idx):\n",
    "    \n",
    "    test_reactions_idx = open('test_reactions.txt').read().split(',')\n",
    "\n",
    "    x_train, x_test = [], []\n",
    "    y_train, y_test = [], []\n",
    "    idx_train, idx_test = [], []\n",
    "    for bag, label, i in zip(bags, labels, idx):\n",
    "        if i in test_idx:\n",
    "            x_test.append(bag)\n",
    "            y_test.append(label)\n",
    "            idx_test.append(i)\n",
    "        else:\n",
    "            x_train.append(bag)\n",
    "            y_train.append(label)\n",
    "            idx_train.append(i)\n",
    "            \n",
    "    x_train, x_test, y_train, y_test = np.array(x_train), np.array(x_test), np.array(y_train), np.array(y_test)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test, idx_train, idx_test\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test, idx_train, idx_test = train_test_split_default(bags, labels, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of generated pmapper descriptors may be quite large, which can hinder model training. A representative set of descriptors can be selected by removing redundant descriptors with rare occurrences. Namely, descriptors with non-zero values in less than N % of the training conformations are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def remove_dsc(bags, tresh_down=0.1, tresh_up=1):\n",
    "    \n",
    "    bags_concat = np.concatenate(bags)\n",
    "    \n",
    "    tresh_down = tresh_down * len(bags_concat)\n",
    "    tresh_up = tresh_up * len(bags_concat)\n",
    "    \n",
    "    out = []\n",
    "    for dsc in range(bags_concat.shape[-1]):\n",
    "        p = sum(np.where(bags_concat[:, dsc] == 0, 0, 1))\n",
    "        if p < tresh_down or p > tresh_up:\n",
    "            out.append(dsc)\n",
    "    bags = [np.delete(bag, out, axis=1) for bag in bags]\n",
    "    return out, np.array(bags)\n",
    "\n",
    "def scale_data(x_train, x_test):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(np.vstack(x_train))\n",
    "    x_train_scaled = x_train.copy()\n",
    "    x_test_scaled = x_test.copy()\n",
    "    for i, bag in enumerate(x_train):\n",
    "        x_train_scaled[i] = scaler.transform(bag)\n",
    "    for i, bag in enumerate(x_test):\n",
    "        x_test_scaled[i] = scaler.transform(bag)\n",
    "    return np.array(x_train_scaled), np.array(x_test_scaled)\n",
    "\n",
    "\n",
    "out_dsc, x_train_selected = remove_dsc(x_train, tresh_down=0.1, tresh_up=1) \n",
    "x_test_selected = np.array([np.delete(x, out_dsc, axis=1) for x in x_test])\n",
    "\n",
    "x_train_scaled, x_test_scaled = scale_data(x_train_selected, x_test_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models were developed with a multi-instance neural network with an attention mechanism , which highlights a few reactive conformations, responsible for observed selectivity and ignores the irrelevant conformations introducing noise in the modeling process. Namely, the attention mechanism assigns each conformation a weight from 0 to 1, determining its importance in terms of predicting catalyst selectivity. The sum of all attention weights equals 1.  \n",
    "In learning process, each instance (conformation descriptor vector) runs through three fully-connected layers with 256, 128, and 64 hidden neurons (**ndim** parameter). Then the learned instance representations inputs to the attention network with 64 hidden neurons (**det_ndim**) and the number of output neurons equal to the number of input instances. The output neurons are followed by a Softmax unit, calculating attention weights for each instance. The learned instance representations are averaged considering the attention weights, resulting in the embedding vector, which is used to predict selectivity.\n",
    "<img src=\"img/attention_net.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One should implement a protocol for optimizing the hyperparameters of the neural network model. Here we assign the optimal hyperparameters found with the *hyperopt* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zankov/catalyst/3D-MIL-QSSR/miqssr/estimators/attention_nets.py:23: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629431274/work/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  d1 = [i[0].nonzero().flatten().tolist() for i in w_new]\n"
     ]
    }
   ],
   "source": [
    "from miqssr.estimators.attention_nets import AttentionNetRegressor\n",
    "\n",
    "ndim = (x_train_selected[0].shape[1], 256, 128, 64) # number of hidden layers and neurons in the main network\n",
    "det_ndim = (64,)                                    # number of hidden layers and neurons in the attention network\n",
    "n_epoch = 100                                       # maximum number of learning epochs\n",
    "lr = 0.001                                          # learning rate\n",
    "weight_decay = 0.1                                  # l2 regularization\n",
    "att_weight_dropout = 0.9                            # attention weights regularization\n",
    "batch_size = 64                                     # batch size\n",
    "init_cuda = True                                    # True if GPU is available\n",
    "\n",
    "\n",
    "net = AttentionNetRegressor(ndim=ndim, det_ndim=det_ndim, init_cuda=init_cuda)\n",
    "net.fit(x_train_selected, y_train, \n",
    "        n_epoch=n_epoch, \n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        dropout=att_weight_dropout,\n",
    "        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determination coefficient (test set): 0.81\n",
      "ΔΔG Mean absolute error (test set): 0.23 kcal/mol\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "y_pred = net.predict(x_test_selected)\n",
    "\n",
    "print(f'Determination coefficient (test set): {r2_score(y_test, y_pred):.2f}')\n",
    "print(f'ΔΔG Mean absolute error (test set): {mean_absolute_error(y_test, y_pred):.2f} kcal/mol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
